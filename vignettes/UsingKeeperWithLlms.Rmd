---
title: "Using Keeper with Large Language Models"
author: "Martijn Schuemie"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    number_sections: yes
    toc: yes
vignette: >
  %\VignetteIndexEntry{Using Keeper with LLMs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(Keeper)
```
# Introduction

This vignette describes how one could use Keeper to generate patient summaries, and have them be reviewed by a large-language model (LLM).

# Running Keeper

As an example, we'll run Keeper on Eunomia. Eunomia is an OHDSI package that contains a tiny simulated dataset in the Common Data Model (CDM). It is mostly focused on NSAIDs and gastrointestinal (GI) bleeding, so we'll use GI bleed as our example outcome.

## Setting up Eunomia

First we must download and install Eunomia:

```{r eval=FALSE}
install.packages("Eunomia")
```

Next, we can obtain connection details to the Eunomia database. (Note: this will download the database from the internet):

```{r}
library(Eunomia)
connectionDetails <- getEunomiaConnectionDetails()
```

We can have the default set of cohorts generated in Eunomia:

```{r}
createCohorts(connectionDetails)
```

## Run Keeper

Next, we run Keeper. We meticulously select concepts for each Keeper category:

```{r warning=FALSE, message=FALSE}
keeper <- createKeeper(
    connectionDetails = connectionDetails,
    databaseId = "Synpuf",
    cdmDatabaseSchema = "main",
    cohortDatabaseSchema = "main",
    cohortTable = "cohort",
    cohortDefinitionId = 3,
    cohortName = "GI Bleed",
    sampleSize = 100,
    assignNewId = TRUE,
    useAncestor = TRUE,
    doi = c(4202064, 192671, 2108878, 2108900, 2002608),
    symptoms = c(4103703, 443530, 4245614, 28779),
    comorbidities = c(81893, 201606, 313217, 318800, 432585, 4027663, 4180790, 4212540,
                      40481531, 42535737, 46271022), 
    drugs = c(904453, 906780, 923645, 929887, 948078, 953076, 961047, 985247, 992956, 
              997276, 1102917, 1113648, 1115008, 1118045, 1118084, 1124300, 1126128, 
              1136980, 1146810, 1150345, 1153928, 1177480, 1178663, 1185922, 1195492, 
              1236607, 1303425, 1313200, 1353766, 1507835, 1522957, 1721543, 1746940, 
              1777806, 19044727, 19119253, 36863425), 
    diagnosticProcedures = c(4087381, 4143985, 4294382, 42872565, 45888171, 46257627), 
    measurements	= c(3000905, 3000963, 3003458, 3012471, 3016251, 3018677, 3020416, 
                     3022217, 3023314, 3024929, 3034426), 
    alternativeDiagnosis = c(24966, 76725, 195562, 316457, 318800, 4096682), 
    treatmentProcedures = c(0), 
    complications =  c(132797, 196152, 439777, 4192647)      
  )
```

The output is a table with one row per person:
```{r}
keeper
```

# LLM review

We can convert the Keeper output to prompts for a LLM, and parse the output of the LLM to get a classification of whether the patient truly had GI bleeding.

## Generating prompts

We need two prompts: the system prompt is a general description of how the LLM should behave. The (main) prompt contains the patient-specific information. First we create settings, then we generate the system prompt:

```{r}
# Use the default settings:
settings <- createPromptSettings()
systempPrompt <- createSystemPrompt(setting = settings, 
                                    diseaseName = "Gastrointestinal bleeding")  
writeLines(systempPrompt)
```

Then, for each patient we can generate the main prompt:
```{r}
row <- keeper[1, ]
prompt <- createPrompt(setting = settings, 
                       diseaseName = "Gastrointestinal bleeding",
                       keeperRow = row)  
writeLines(prompt)
```

## Generate a response

Next, we can use the system prompt and prompt together to query the LLM. Many LLMs are available, including open source ones. Setting up and interacting with the LLM is beyond the scope of this vignette. Here we assume a LLM was used to generate this response:

```{r}
response <- "Summary: yes"
```

## Parse the response

LLMs can be quite verbose, and often we just want a yes or no answer. For this we can use the `parseLlmResponse()` function that uses a set of patterns to decide between 'yes', 'no', or 'I don't know':

```{r}
parseLlmResponse(response)
```
